{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "import joblib\n",
        "\n",
        "# Suppress specific warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='sklearn')\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_FILE_PATH = 'SyntheticData_Training.csv'\n",
        "MODEL_OUTPUT_PATH = 'pediatric_sepsis_mortality_model.joblib'\n",
        "TARGET_VARIABLE = 'inhospital_mortality'\n",
        "\n",
        "# --- Evaluation Metric Functions ---\n",
        "def calculate_ece(y_true, y_prob, n_bins=10):\n",
        "    \"\"\"\n",
        "    Calculates the Expected Calibration Error (ECE).\n",
        "    Args:\n",
        "        y_true (array-like): True binary labels.\n",
        "        y_prob (array-like): Predicted probabilities for the positive class.\n",
        "        n_bins (int): Number of bins to use for calibration.\n",
        "    Returns:\n",
        "        float: The Expected Calibration Error.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "\n",
        "    # Sort by probability\n",
        "    idx = np.argsort(y_prob)\n",
        "    y_true = y_true[idx]\n",
        "    y_prob = y_prob[idx]\n",
        "\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    ece = 0.0\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        bin_start = bin_boundaries[i]\n",
        "        bin_end = bin_boundaries[i+1]\n",
        "\n",
        "        # Find samples in the current bin\n",
        "        # For the last bin, include 1.0\n",
        "        if i < n_bins -1:\n",
        "            in_bin = (y_prob >= bin_start) & (y_prob < bin_end)\n",
        "        else:\n",
        "            in_bin = (y_prob >= bin_start) & (y_prob <= bin_end)\n",
        "\n",
        "        if np.sum(in_bin) > 0:\n",
        "            # Average predicted probability in this bin\n",
        "            avg_confidence_in_bin = np.mean(y_prob[in_bin])\n",
        "            # Fraction of positives in this bin\n",
        "            accuracy_in_bin = np.mean(y_true[in_bin])\n",
        "\n",
        "            delta = np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
        "            ece += delta * (np.sum(in_bin) / len(y_true)) # Weight by proportion of samples in bin\n",
        "\n",
        "    return ece\n",
        "\n",
        "def calculate_net_benefit(y_true, y_pred_proba, thresholds):\n",
        "    \"\"\"\n",
        "    Calculates Net Benefit for a range of probability thresholds.\n",
        "    This is a simplified conceptual version. A full decision curve analysis is more involved.\n",
        "    Args:\n",
        "        y_true (array-like): True binary labels.\n",
        "        y_pred_proba (array-like): Predicted probabilities for the positive class.\n",
        "        thresholds (array-like): Array of probability thresholds to evaluate.\n",
        "    Returns:\n",
        "        dict: A dictionary with thresholds as keys and Net Benefit as values.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred_proba = np.asarray(y_pred_proba)\n",
        "    net_benefits = {}\n",
        "\n",
        "    n = len(y_true)\n",
        "    if n == 0:\n",
        "        return {t: 0 for t in thresholds}\n",
        "\n",
        "    for pt in thresholds:\n",
        "        # Classify based on threshold\n",
        "        y_pred_binary = (y_pred_proba >= pt).astype(int)\n",
        "\n",
        "        tp = np.sum((y_pred_binary == 1) & (y_true == 1))\n",
        "        fp = np.sum((y_pred_binary == 1) & (y_true == 0))\n",
        "\n",
        "        # Net Benefit formula: (TP/N) - (FP/N) * (pt / (1-pt))\n",
        "        # Avoid division by zero if pt is 1; net benefit is undefined or handled as limit\n",
        "        if pt == 1.0:\n",
        "            # If threshold is 1, we only treat if predicted prob is 1.\n",
        "            # If no one is predicted as 1, TP and FP are 0.\n",
        "            # If pt=1, (pt/(1-pt)) is infinite. Net benefit is typically -infinity unless FP=0.\n",
        "            # A common convention is to not calculate for pt=1 or handle it carefully.\n",
        "            # For simplicity, if pt=1 and FP > 0, it's highly negative. If FP=0, it's TP/N.\n",
        "            if fp > 0:\n",
        "                 net_benefit_pt = -np.inf\n",
        "            else:\n",
        "                 net_benefit_pt = tp / n\n",
        "        elif pt == 0.0:\n",
        "             # If threshold is 0, everyone is treated. TP = total positives, FP = total negatives.\n",
        "             # (pt/(1-pt)) is 0. Net benefit = TP/N.\n",
        "             # TP here is total number of actual positives if all are treated.\n",
        "             net_benefit_pt = np.sum(y_true == 1) / n\n",
        "        else:\n",
        "            net_benefit_pt = (tp / n) - (fp / n) * (pt / (1 - pt))\n",
        "\n",
        "        net_benefits[pt] = net_benefit_pt\n",
        "    return net_benefits\n",
        "\n",
        "# --- Main Script ---\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the ML pipeline.\n",
        "    \"\"\"\n",
        "    print(\"Starting Pediatric Sepsis Mortality Prediction Model Training...\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    print(f\"\\n[1] Loading data from {DATA_FILE_PATH}...\")\n",
        "    if not os.path.exists(DATA_FILE_PATH):\n",
        "        print(f\"ERROR: Data file not found at {DATA_FILE_PATH}. Please ensure it's in the correct location.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        data = pd.read_csv(DATA_FILE_PATH)\n",
        "        print(f\"Data loaded successfully. Shape: {data.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Identify Target and Features, and Exclude Variables\n",
        "    print(\"\\n[2] Identifying target, features, and excluding specified variables...\")\n",
        "    if TARGET_VARIABLE not in data.columns:\n",
        "        print(f\"ERROR: Target variable '{TARGET_VARIABLE}' not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    y = data[TARGET_VARIABLE]\n",
        "\n",
        "    # Variables to exclude based on the data dictionary and challenge instructions\n",
        "    intervention_vars = [f'admitabx_adm___{i}' for i in range(1, 22)]\n",
        "    # Ensure correct spelling for symptoms_adm___17 (data dictionary had a typo 'symtoms_adm___17')\n",
        "    removed_vars_update = ['cookfuel_adm___8', 'symptoms_adm___17', 'symtoms_adm___17']\n",
        "    other_excluded = ['studyid_adm', 'lengthadm'] # 'lengthadm' is an outcome, potential leakage\n",
        "\n",
        "    columns_to_drop = intervention_vars + removed_vars_update + other_excluded\n",
        "\n",
        "    # Drop only existing columns to avoid KeyErrors\n",
        "    actual_columns_to_drop = [col for col in columns_to_drop if col in data.columns]\n",
        "    X = data.drop(columns=[TARGET_VARIABLE] + actual_columns_to_drop, errors='ignore')\n",
        "\n",
        "    print(f\"Target variable: '{TARGET_VARIABLE}'\")\n",
        "    print(f\"Number of features before selection: {X.shape[1]}\")\n",
        "    print(f\"Dropped columns: {actual_columns_to_drop}\")\n",
        "\n",
        "\n",
        "    # 3. Define Numerical and Categorical Features\n",
        "    # CRITICAL STEP: These lists are based on the provided data dictionary snippet and common interpretations.\n",
        "    # They MUST be thoroughly reviewed and validated against the complete data dictionary and dataset.\n",
        "    print(\"\\n[3] Defining numerical and categorical features...\")\n",
        "\n",
        "    numerical_features = [\n",
        "        'agecalc_adm', 'height_cm_adm', 'weight_kg_adm', 'muac_mm_adm', 'hr_bpm_adm',\n",
        "        'rr_brpm_app_adm', 'sysbp_mmhg_adm', 'diasbp_mmhg_adm', 'temp_c_adm',\n",
        "        'spo2site1_pc_oxi_adm', 'spo2site2_pc_oxi_adm', 'spo2other_adm', 'momage_adm',\n",
        "        'momagefirstpreg_adm', 'householdsize_adm', 'alivechildren_adm', 'deadchildren_adm',\n",
        "        'hematocrit_gpdl_adm', 'lactate_mmolpl_adm', 'lactate2_mmolpl_adm',\n",
        "        'glucose_mmolpl_adm', 'sqi1_perc_oxi_adm', 'sqi2_perc_oxi_adm'\n",
        "    ]\n",
        "\n",
        "    categorical_features = [\n",
        "        'sex_adm', 'spo2onoxy_adm', 'oxygenavail_adm', 'respdistress_adm', 'caprefill_adm',\n",
        "        'bcseye_adm', 'bcsmotor_adm', 'bcsverbal_adm', 'bcgscar_adm', 'vaccmeasles_adm',\n",
        "        'vaccmeaslessource_adm', 'vaccpneumoc_adm', 'vaccpneumocsource_adm', 'vaccdpt_adm',\n",
        "        'vaccdptsource_adm', 'priorweekabx_adm', 'priorweekantimal_adm',\n",
        "        'symptoms_adm___1', 'symptoms_adm___2', 'symptoms_adm___3', 'symptoms_adm___4',\n",
        "        'symptoms_adm___5', 'symptoms_adm___6', 'symptoms_adm___7', 'symptoms_adm___8',\n",
        "        'symptoms_adm___9', 'symptoms_adm___10', 'symptoms_adm___11', 'symptoms_adm___12',\n",
        "        'symptoms_adm___13', 'symptoms_adm___14', 'symptoms_adm___15', 'symptoms_adm___16',\n",
        "        'symptoms_adm___18', # symptoms_adm___17 is excluded\n",
        "        'comorbidity_adm___1', 'comorbidity_adm___2', 'comorbidity_adm___3', 'comorbidity_adm___4',\n",
        "        'comorbidity_adm___5', 'comorbidity_adm___6', 'comorbidity_adm___7', 'comorbidity_adm___8',\n",
        "        'comorbidity_adm___9', 'comorbidity_adm___10', 'comorbidity_adm___11', 'comorbidity_adm___12',\n",
        "        'priorhosp_adm', 'prioryearwheeze_adm', 'prioryearcough_adm', 'diarrheaoften_adm',\n",
        "        'tbcontact_adm', 'feedingstatus_adm', 'exclbreastfed_adm', 'nonexclbreastfed_adm',\n",
        "        'totalbreastfed_adm', 'deliveryloc_adm', 'birthattend_adm', 'duedateknown_adm',\n",
        "        'birthdetail_adm___1', 'birthdetail_adm___2', 'birthdetail_adm___3',\n",
        "        'birthdetail_adm___4', 'birthdetail_adm___5', 'birthdetail_adm___6',\n",
        "        'travelmethod_adm', 'traveldist_adm', 'badhealthduration_adm', 'caregiver_adm_new',\n",
        "        'caregiverage_adm', 'caregivermarried_adm', 'momalive_adm', 'momageknown_adm',\n",
        "        'momagefirstpregknown_adm', 'momedu_adm', 'momhiv_adm', 'watersource_adm', 'waterpure_adm',\n",
        "        'cookfuel_adm___1', 'cookfuel_adm___2', 'cookfuel_adm___3', 'cookfuel_adm___4',\n",
        "        'cookfuel_adm___5', 'cookfuel_adm___6', 'cookfuel_adm___7', # cookfuel_adm___8 is excluded\n",
        "        'cookloc_adm', 'lightfuel_adm', 'tobacco_adm', 'bednet_adm',\n",
        "        'hctpretransfusion_adm', 'hivstatus_adm', 'malariastatuspos_adm'\n",
        "    ]\n",
        "\n",
        "    # Filter features to only those present in X\n",
        "    numerical_features = [f for f in numerical_features if f in X.columns]\n",
        "    categorical_features = [f for f in categorical_features if f in X.columns]\n",
        "\n",
        "    # Check for overlap or missing features\n",
        "    all_defined_features = set(numerical_features + categorical_features)\n",
        "    all_X_columns = set(X.columns)\n",
        "\n",
        "    if all_defined_features != all_X_columns:\n",
        "        print(\"\\nWARNING: Feature set mismatch!\")\n",
        "        missing_in_defined = all_X_columns - all_defined_features\n",
        "        if missing_in_defined:\n",
        "            print(f\"  Features in X but not in defined lists (will be dropped by ColumnTransformer if remainder='drop'): {missing_in_defined}\")\n",
        "        extra_in_defined = all_defined_features - all_X_columns\n",
        "        if extra_in_defined:\n",
        "            print(f\"  Features in defined lists but not in X (were likely dropped or misnamed): {extra_in_defined}\")\n",
        "\n",
        "    print(f\"Selected {len(numerical_features)} numerical features.\")\n",
        "    print(f\"Selected {len(categorical_features)} categorical features.\")\n",
        "    print(f\"Total features for model: {len(numerical_features) + len(categorical_features)}\")\n",
        "\n",
        "\n",
        "    # 4. Create Preprocessing Pipelines\n",
        "    print(\"\\n[4] Creating preprocessing pipelines...\")\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # Can also use a constant fill_value\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='drop' # Drop any columns not explicitly handled\n",
        "    )\n",
        "\n",
        "    # 5. Split Data\n",
        "    print(\"\\n[5] Splitting data into training and testing sets...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y # Important for imbalanced datasets\n",
        "    )\n",
        "    print(f\"Training set shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "    print(f\"Test set shape: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "\n",
        "    # 6. Define and Train Model\n",
        "    # Using Logistic Regression as a starting point.\n",
        "    # Consider RandomForestClassifier or GradientBoostingClassifier for potentially better performance.\n",
        "    print(\"\\n[6] Defining and training Logistic Regression model...\")\n",
        "    model_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        model_pipeline.fit(X_train, y_train)\n",
        "        print(\"Model training complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model training: {e}\")\n",
        "        # Potentially print more details about which features might be causing issues\n",
        "        # For example, if a numerical feature is actually all NaNs after selection,\n",
        "        # or if a categorical feature has an unexpected dtype.\n",
        "        # You can try to fit the preprocessor alone to debug:\n",
        "        # try:\n",
        "        #     X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "        #     print(f\"Preprocessor fit_transform successful on X_train. Shape: {X_train_transformed.shape}\")\n",
        "        # except Exception as pe:\n",
        "        #     print(f\"Error during preprocessor fitting: {pe}\")\n",
        "        return\n",
        "\n",
        "    # 7. Evaluate Model\n",
        "    print(\"\\n[7] Evaluating model...\")\n",
        "    try:\n",
        "        y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]\n",
        "        y_pred_binary = model_pipeline.predict(X_test)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model prediction: {e}\")\n",
        "        return\n",
        "\n",
        "    # AUC-ROC\n",
        "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(f\"  Area Under the ROC Curve (AUC-ROC): {auc_roc:.4f}\")\n",
        "\n",
        "    # AUPRC\n",
        "    auprc = average_precision_score(y_test, y_pred_proba)\n",
        "    print(f\"  Area Under the Precision-Recall Curve (AUPRC): {auprc:.4f}\")\n",
        "\n",
        "    # Estimated Calibration Error (ECE)\n",
        "    ece = calculate_ece(y_test, y_pred_proba)\n",
        "    print(f\"  Estimated Calibration Error (ECE): {ece:.4f}\")\n",
        "\n",
        "    # Net Benefit (Conceptual - requires careful threshold selection and interpretation)\n",
        "    # Define clinically relevant thresholds for decision making\n",
        "    # Example thresholds:\n",
        "    nb_thresholds = np.linspace(0.05, 0.95, 10)\n",
        "    net_benefits = calculate_net_benefit(y_test, y_pred_proba, nb_thresholds)\n",
        "    print(f\"  Net Benefit (conceptual):\")\n",
        "    for thresh, nb_val in net_benefits.items():\n",
        "        print(f\"    Threshold {thresh:.2f}: NB = {nb_val:.4f}\")\n",
        "    print(\"    Note: Net Benefit calculation here is illustrative. Full Decision Curve Analysis is recommended.\")\n",
        "\n",
        "\n",
        "    # 8. Save Trained Model\n",
        "    print(f\"\\n[8] Saving trained model to {MODEL_OUTPUT_PATH}...\")\n",
        "    try:\n",
        "        joblib.dump(model_pipeline, MODEL_OUTPUT_PATH)\n",
        "        print(f\"Model saved successfully to {MODEL_OUTPUT_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "    print(\"\\n--- Script Finished ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FchDuQ2DfG4l"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}