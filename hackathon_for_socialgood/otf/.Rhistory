#accuracy of the regression tree model
caret::confusionMatrix(data=predict(model_tree, type = "class"),
reference = df_train$target,
positive="0")
#accuracy improvement
penalty_matrix <- matrix(c(0, 1, 10, 0), byrow = TRUE, nrow = 2)
model_tree_penalty <- rpart(target ~ ., data = df_train, method = "class",
parms = list(loss = penalty_matrix))
caret::confusionMatrix(data=predict(model_tree_penalty, type = "class"),
reference = df_train$target,
positive="0")
model_tree <- rpart(target ~ ., data = df_train,
method = "class", cp = 0.00001)
caret::confusionMatrix(data=predict(model_tree_penalty, type = "class"),
reference = df_train$target,
positive="0")
library(unbalanced)
#remove missing data
df_train <- na.omit(train)
df_test <- na.omit(test)
df_train[1] <- NULL #drop id column
#make each variable a factor
df_train <- df_train %>% map_df(function(.x) as.factor(.x))
df_test <- df_test %>% map_df(function(.x) as.factor(.x))
n<-ncol(df_train)
output<- df_train$target
output<-as.factor(output)
input<- df_train[ ,-n]
View(input)
#Balance the Dataset using ubSMOTE#
data<-ubBalance(X= input, Y=output, type="ubSMOTE", percOver=300, percUnder=150, verbose=TRUE)
#drop nom columns
df_train['nom_5'] <- NULL
df_train['nom_6'] <- NULL
df_train['nom_7'] <- NULL
df_train['nom_8'] <- NULL
df_train['nom_9'] <- NULL
#check for data imbalance in response variable
round(prop.table(table(df_train$target)), 2) #70-30
#fix data imbalance
n<-ncol(df_train)
output<- df_train$target
output<-as.factor(output)
input<- df_train[ ,-n]
View(input)
#Balance the Dataset using ubSMOTE#
data<-ubBalance(X= input, Y=output, type="ubSMOTE", percOver=300, percUnder=150, verbose=TRUE)
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools',
'dummies','readxl','cluster','factoextra','psy','lattice','nFactors',
'scales','NbClust','caretEnsemble','SuperLearner','rpart','unbalanced','tibble',
'DMwR','ROSE')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load data
train <- read.csv(file.choose())
test <- read.csv(file.choose())
df_train <- na.omit(train)
df_test <- na.omit(test)
#data review
df_train[1] <- NULL #drop id column
#make each variable a factor
df_train <- df_train %>% map_df(function(.x) as.factor(.x))
df_test <- df_test %>% map_df(function(.x) as.factor(.x))
#drop nom columns
df_train['nom_5'] <- NULL
df_train['nom_6'] <- NULL
df_train['nom_7'] <- NULL
df_train['nom_8'] <- NULL
df_train['nom_9'] <- NULL
df_train['ord_3'] <- NULL
df_train['ord_4'] <- NULL
df_train['ord_5'] <- NULL
df_test['nom_5'] <- NULL
df_test['nom_6'] <- NULL
df_test['nom_7'] <- NULL
df_test['nom_8'] <- NULL
df_test['nom_9'] <- NULL
df_test['ord_3'] <- NULL
df_test['ord_4'] <- NULL
df_test['ord_5'] <- NULL
Target <- df_train$target
#categorical variables
df_train_cat <- df_train[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,14)]
df_test_cat <- df_test[,c(2,3,4,5,6,7,8,9,10,11,12,13,14,15)]
#one hot encoding
df_train_cat_new <- dummy.data.frame(as.data.frame(df_train_cat), sep = "_")
df_test_cat_new <- dummy.data.frame(as.data.frame(df_test_cat), sep = "_")
#combine data
df_train_new <- cbind(df_train_cat_new,Target)
df_train_new <- as.data.frame(df_train_new)
View(df_train_new)
glimpse(df_train_new)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=5)
fit.gbm <- train(Target~., data=df_train_new, method="gbm", trControl=control)
#combine data
df_train_new <- cbind(as.data.frame(df_train_cat_new),Target)
df_train_new <- as.data.frame(df_train_new)
fit.rf <- train(Target~., data=df_train_new, method="rf", trControl=control)
typeof(df_train_new)
df_train_new <- data.frame(df_train_new)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=5)
#random forest
fit.rf <- train(Target~., data=df_train_new, method="rf", trControl=control)
fit.rf <- caret::train(Target~., data=df_train_new, method="rf", trControl=control)
LIBRARY(glmnet)
library(glmnet)
y <- train$target
tr <- train[,1:24]
tre <- rbind(tr,test)
tre <- tre %>%
mutate(month = as.character(month),
day = as.character(day),
ord_0 = as.character(ord_0),
bin_2 = as.character(bin_2),
bin_1 = as.character(bin_1),
bin_0 = as.character(bin_0))
tre_0 <- sparse.model.matrix(~. ,data=tre[,2:24])
tre_0 <- tre_0[,2:16530]
pf_0 <- colSums(tre_0)
tre_0<- tre_0[,pf_0>5]
tr_0 <- tre_0[1:nrow(tr),]
te_0 <- tre_0[(nrow(tr)+1):(nrow(tr)+nrow(test)),]
pf <- colSums(tr_0)
set.seed(1234)
m_cv <- cv.glmnet(tr_0,y, family = "binomial",type.measure = "auc",alpha=0,
standardize = F, lambda=seq(0.000035,0.000030,-0.0000002),
penalty.factor=(1/pf)^0.1, thresh = 1e-10, maxit = 1e9)
pr <- predict(m_cv$glmnet.fit, te_0, s= m_cv$lambda.min * 0.9,  type = "response")
colnames(pr) <- "target"
submission <- data.frame(id = tes$id, target = pr, row.names = NULL)
submission <- data.frame(id = test$id, target = pr, row.names = NULL)
write.csv(submission, file="submission_glmnet.csv", row.names = FALSE)
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools',
'dummies','stringr','forcats')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice',
'caTools','dummies','ggfortify)')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
users <- read_csv("BX-Users_clean.csv")
books <- read_csv("BX-Books_clean.csv")
ratings <- read_csv("BX-Book-Ratings_clean.csv")
setwd("~/Documents/Coding/R/shiny/bookRecommender")
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice',
'caTools','dummies','ggfortify)')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
users <- read_csv("BX-Users_clean.csv")
books <- read_csv("BX-Books_clean.csv")
ratings <- read_csv("BX-Book-Ratings_clean.csv")
View(books)
View(ratings)
users <- read_csv("BX-Users_clean.csv")
View(users)
View(ratings)
View(books)
install.packages("shinythemes")
shiny::runApp()
runApp('Documents/Coding/R/shiny/bookRecommender')
runApp('Documents/Coding/R/shiny/bookRecommender')
runApp('Documents/Coding/R/shiny/bookRecommender')
runApp('Documents/Coding/R/shiny/bookRecommender')
shiny::runApp('Documents/Coding/R/shiny/bookRecommender')
runApp('Documents/Coding/R/shiny/bookRecommender')
runApp('Documents/Coding/R/shiny/bookRecommender')
runApp('Documents/Coding/R/shiny/bookRecommender')
install.packages ("pwr")
library (pwr)
######## 2-sample test for equality of proportions ############
prop.test(c (225, 250), c (3450, 3000))
install.packages (“bayesAB”)
library (bayesAB)
######## 2-sample test for equality of proportions ############
prop.test(c (225, 250), c (3450, 3000))
A_binom <- rbinom (3450, 1, 0.065)
B_binom <- rbinom (3000, 1, 0.083)
plotBeta (1, 1)
plotBeta (100, 200) ## more specific range of p
AB1 <- bayesTest (A_binom, B_binom,
priors = c ('alpha' = 1, 'beta' = 1),
distribution = 'bernoulli')
AB2 <- bayesTest (A_binom, B_binom,
priors = c ('alpha' = 100,'beta' = 200),
distribution = 'bernoulli')
install.packages (“bayesAB”)
library (bayesAB)
uniqueTitle <- unique(books$bookTitle)
uniqueAuthor <- unique(books$bookAuthor)
uniquePublisher <- unique(books$publisher)
uniqueYear <- unique(books$yearOfPublication)
View(books)
View(ratings)
View(users)
View(books)
shiny::runApp('Documents/Coding/R/shiny/bookRecommender')
runApp('Documents/Coding/R/shiny/bookRecommender')
View(books)
View(ratings)
bookRating <- ratings %>%
inner_join(books,by = "ISBN")
View(bookRating)
bookRating <- ratings %>%
inner_join(books,by = "ISBN") %>%
select (bookTitle, bookRating)
View(bookRating)
#aggregate book ratings
bookRating2 <- bookRating %>%
group_by(bookTitle) %>%
summarise(bookRatingTotal = sum(bookRating))
View(bookRating2)
#aggregate book ratings
bookRating2 <- bookRating %>%
group_by(bookTitle) %>%
summarise(bookRatingTotal = average(bookRating))
#aggregate book ratings
bookRating2 <- bookRating %>%
group_by(bookTitle) %>%
summarise(bookRatingTotal = mean(bookRating))
View(bookRating2)
#aggregate book ratings
bookRating2 <- bookRating %>%
group_by(bookTitle) %>%
summarise(bookRatingTotal = sum(bookRating), n=n())
View(bookRating2)
bookRating2$AverageRate <- bookRating2$bookRatingTotal/bookRating2$n
View(bookRating2)
bookRating <- ratings %>%
inner_join(books,by = "ISBN") %>%
select (bookTitle, bookAuthor, bookRating)
#aggregate book ratings
bookRating2 <- bookRating %>%
group_by(bookTitle, bookAuthor) %>%
summarise(bookRatingTotal = sum(bookRating), n=n())
View(bookRating2)
bookRating2$AverageRate <- bookRating2$bookRatingTotal/bookRating2$n
View(bookRating2)
#top 5 book ratings visualization
p<-ggplot(data=bookRating2, aes(x=bookAuthor, y=AverageRate)) +
geom_bar(stat="identity")
# Horizontal bar plot
p + coord_flip()
bookRatingTop5 <- bookRating2 %>% top_n(5)
#top 5 book ratings visualization
p<-ggplot(data=bookRatingTop5, aes(x=bookAuthor, y=AverageRate)) +
geom_bar(stat="identity")
# Horizontal bar plot
p + coord_flip()
View(bookRatingTop5)
View(bookRating2)
bookRatingTop5 <- bookRating2 %>%
select(bookTitle, bookAuthor, AverageRate) %>%
tally(AverageRate) %>%
top_n(5)
View(bookRatingTop5)
bookRatingTop5 <- bookRating2 %>%
select(bookTitle, bookAuthor, AverageRate) %>%
arrange(desc(AverageRate)) %>%
top_n(5)
View(bookRatingTop5)
bookRatingTop5 %>% top_n(5)
bookRatingTop5 <- bookRatingTop5 %>% top_n(5)
bookRatingTop5 <- bookRating2 %>%
arrange(desc(AverageRate)) %>%
select(bookTitle, bookAuthor, AverageRate) %>%
top_n(5)
bookRatingTop5 <- bookRating2 %>%
arrange(desc(AverageRate)) %>%
top_n(5) %>%
select(bookTitle, bookAuthor, AverageRate)
bookRatingTop5 <- bookRating2 %>%
arrange(desc(AverageRate)) %>%
head(n = 10L) %>%
select(bookTitle, bookAuthor, AverageRate)
View(bookRatingTop5)
#top 5 book ratings visualization
p<-ggplot(data=bookRatingTop, aes(x=bookTitle, y=AverageRate)) +
geom_bar(stat="identity")
# Horizontal bar plot
p + coord_flip()
bookRatingTop <- bookRating2 %>%
arrange(desc(AverageRate)) %>%
head(n = 10L) %>%
select(bookTitle, bookAuthor, AverageRate) %>%
#top 5 book ratings visualization
p<-ggplot(data=bookRatingTop, aes(x=bookTitle, y=AverageRate)) +
geom_bar(stat="identity")
# Horizontal bar plot
p + coord_flip()
bookRatingTop <- bookRating2 %>%
arrange(desc(AverageRate)) %>%
head(n = 10L) %>%
select(bookTitle, bookAuthor, AverageRate)
#top 5 book ratings visualization
p<-ggplot(data=bookRatingTop, aes(x=bookTitle, y=AverageRate)) +
geom_bar(stat="identity")
# Horizontal bar plot
p + coord_flip()
bookPublishing <- books %>%
select (bookAuthor, yearOfPublication) %>%
group_by(yearOfPublication) %>%
tally()
View(bookPublishing)
bookPublishing <- books %>%
select (bookAuthor, yearOfPublication) %>%
group_by(bookAuthor,yearOfPublication) %>%
tally() %>%
arrange(desc(n)) %>%
head(n = 10L)
View(bookPublishing)
p<-ggplot(data=bookPublishing, aes(x=yearOfPublication, y=n)) +
geom_bar(stat="identity")
p
View(users)
View(ratings)
bookAge <- ratings %>%
inner_join(books, by="ISBN") %>%
inner_join(users, by="userID") %>%
select(bookAuthor, Age)
View(bookAge)
# Basic histogram
ggplot(bookAge, aes(x=Age)) + geom_histogram()
# Change the width of bins
# Basic histogram
ggplot(bookAge, aes(x=Age)) + geom_histogram()
# Change the width of bins
ggplot(df, aes(x=weight)) +
geom_histogram(binwidth=1)
# Change the width of bins
ggplot(bookAge, aes(x=Age)) +
geom_histogram(binwidth=1)
# Basic histogram
ggplot(bookAge, aes(x=Age)) + geom_histogram()
# Change the width of bins
ggplot(bookAge, aes(x=Age)) +
geom_histogram(binwidth=10)
ggplot(bookAge, aes(x=Age)) +
geom_histogram(binwidth=25)
bookLocation <- ratings %>%
inner_join(books, by="ISBN") %>%
inner_join(users, by="userID") %>%
select(bookAuthor, Location)
View(bookLocation)
bookLocation2 <- separate(bookLocation, Location,
into = c("area", "location","country"), sep = 3)
bookLocation2 <- separate(bookLocation, Location,
into = c("area", "location","country"), sep = 2)
bookLocation2 <- separate(bookLocation, Location,
into = c("area", "location","country"), sep = ",")
View(bookLocation2)
View(bookLocation2)
bookLocation3 <- bookLocation2 %>%
group_by(bookAuthor, country) %>%
tally()
View(bookLocation3)
shiny::runApp('Documents/Coding/R/shiny/KaggleSurvey')
shiny::runApp('Documents/Coding/R/shiny/KaggleSurvey')
#
# This is a Shiny web application that uses the Kaggle 2019 Data
# Science and Machine learning survery
#
#remove old data
#rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice',
'caTools','dummies','ggfortify','shiny')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load data
multipleChoice <- read_csv("multiple_choice_responses.csv")
otherText <- read_csv("other_text_responses.csv")
questions <- read_csv("questions_only.csv")
surveySchema<- read_csv("survey_schema.csv")
shiny::runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
shiny::runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
runApp('Documents/Coding/R/shiny/wine_review')
library(tensorflow)
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','shiny',
'countrycode','shinydashboard','highcharter',"gridExtra")
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load data
df <- read.csv("otf.csv")
setwd("~/Documents/Coding/hackathon_coding_challenges/hackathon_for_socialgood/otf")
#load data
df <- read.csv("otf.csv")
data_regression_df <- df %>%
select(year,grant_program2,co_application,budget_fund,program_area,planned_dates,amount_awarded)
#backup data
df.backup <- data_regression_df
data_regression_df$year <- as.factor(data_regression_df$year)
data_regression_df$planned_dates <- as.factor(data_regression_df$planned_dates)
Target <- df$amount_awarded
df_cat <- data_regression_df[,1:6]
#df_cts <- df[,2:8]
library(dummies)
#one hot encoding
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
#combine data
df_new <- cbind(df_cat_new,Target)
install.packages('dummies')
Target <- df$amount_awarded
df_cat <- data_regression_df[,1:6]
#df_cts <- df[,2:8]
library(dummies)
#one hot encoding
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
#combine data
df_new <- cbind(df_cat_new,Target)
install.packages('rsample')
library(rsample)
set.seed(123)
data_split <- initial_split(df_new, prop = 4/5, strata = NULL)
training_data <- training(data_split)
testing_data <- testing(data_split)
install.packages("recipes")
rec = recipe(Target ~ ., data = training_data) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
prep()
library(recipes)
rec = recipe(Target ~ ., data = training_data) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
prep()
install.packages(c("tensorflow","keras"))
library(tensorflow)
library(keras)
build_model = function(){
model = keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(training_data)-1) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>%
compile(loss = "rmse",
optimizer = "rmsprop",
metrics = list("mae","mse"))
return(model)
}
model1 = build_model()
y
install.packages("keras")
install.packages("keras")
install_tensorflow()
install_keras()
build_model = function(){
model = keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(training_data)-1) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>%
compile(loss = "rmse",
optimizer = "rmsprop",
metrics = list("mae","mse"))
return(model)
}
model1 = build_model()
