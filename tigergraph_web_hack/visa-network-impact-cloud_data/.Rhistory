View(df)
df <- df %>%
select(reviewer_location, review_total_contributions,review_upvotes, helpful_upvotes,
date_of_experience, review_language, review_text) %>%
na.omit()
#load data
df <- read.csv(file.choose(),sep = ",")
glimpse(df)
#drop columns
df <- df%>%
select(-c(X,X.1,X.2,X.3))
#columns to drop attraction_name, category, reviewer_name,
df <- df %>%
select(reviewer_location, review_total_contributions,review_upvotes,
date_of_experience, review_language, review_text) %>%
na.omit()
#load data
df <- read.csv(file.choose(),sep = ",")
glimpse(df)
#drop columns
df <- df%>%
select(-c(X,X.1,X.2,X.3))
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#check columns
unique(df$attraction_name)
unique(df$category)
#columns to drop attraction_name, category, reviewer_name,
df <- df %>%
select(reviewer_location, review_total_contributions,
date_of_experience, review_language, review_text) %>%
na.omit()
glimpse(df)
#load data
df <- read.csv(file.choose(),sep = ",")
glimpse(df)
#drop columns
df <- df%>%
select(-c(X,X.1,X.2,X.3))
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#check columns
unique(df$attraction_name)
unique(df$category)
#columns to drop attraction_name, category, reviewer_name,
df <- df %>%
select(reviewer_location, review_total_contributions,
date_of_experience, review_language, review_text, rating) %>%
na.omit()
table(df$rating)
glimpse(df)
df$date_of_experience <- as.Date(df$date_of_experience)
glimpse(df)
#load data
df <- read.csv(file.choose(),sep = ",")
glimpse(df)
#drop columns
df <- df%>%
select(-c(X,X.1,X.2,X.3))
View(df)
df <- df%>%
select(-c(X,X.1,X.2,X.3))
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#check columns
unique(df$attraction_name)
unique(df$category)
#columns to drop attraction_name, category, reviewer_name,
df <- df %>%
select(reviewer_location, review_total_contributions,
date_of_experience, review_language, review_text, rating) %>%
na.omit()
df$date_of_experience <- lubridate::mdy(df$date_of_experience)
View(df)
df$year <- lubridate::year(df$date_of_experience)
df$month <- lubridate::month(df$date_of_experience)
View(df)
View(df)
review_ratings_df <- df %>%
select(review_text, rating)
cts_df <- df %>%
select(review_total_contributions)
cat_df <- df %>%
select(reviewer_location, review_language)
other_cat_df <- df %>%
select(year,month)
#Label Encoder
labelEncoder <-function(x){
as.numeric(factor(x))-1
}
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
cts_df <- as.data.frame(lapply(cts_df, normalize))
cat_df  <- as.data.frame(lapply(cat_df , labelEncoder))
new_df <- cbind(cat_df,cts_df,other_cat_df,review_ratings_df)
#split into train and test
set.seed(123)
sample <- sample.split(new_df,SplitRatio = 0.75)
train <- subset(new_df,sample ==TRUE)
test <- subset(new_df, sample==FALSE)
View(new_df)
train_df <- train %>%
select(review_text, rating)
test_df <- test %>%
select(review_text, rating)
clean_data <- function(df){
corpus_df <- VCorpus(VectorSource(df$review_text))
##Removing Punctuation
corpus_df <- tm_map(corpus_df, content_transformer(removePunctuation))
##Removing numbers
corpus_df <- tm_map(corpus_df, removeNumbers)
##Converting to lower case
#corpus_df <- tm_map(corpus_df, content_transformer(tolower))
##Removing stop words
corpus_df <- tm_map(corpus_df, content_transformer(removeWords), stopwords('english'))
##Stemming
corpus_df <- tm_map(corpus_df, stemDocument)
##Whitespace
corpus_df <- tm_map(corpus_df, stripWhitespace)
# Create Document Term Matrix
dtm_df <- DocumentTermMatrix(corpus_df)
corpus_df <- removeSparseTerms(dtm_df, 0.8)
dtm_df_matrix <- as.matrix(corpus_df)
dtm_df_matrix <- cbind(dtm_df_matrix,df$Rating)
colnames(dtm_df_matrix)[ncol(dtm_df_matrix)] <- "y"
final_df <- as.data.frame(dtm_df_matrix)
final_df$y <- as.factor(final_df$y)
return (final_df)
}
train_df <- clean_data(train_df)
test_df <- clean_data(test_df)
View(train)
View(train_df)
train <- cbind(train[,c(1,2,3,4,5)], train_df)
test <- cbind(test[,c(1,2,3,4,5)], test_df)
#model training
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs = FALSE)
#glm
fit.glm <- train(as.factor(y)~., data=train, method="multinom", metric = "Accuracy",
trControl = control)
#random forest
fit.rf <- train(as.factor(y)~., data=train, method="rf", metric = "Accuracy",
trControl = control)
#boosting algorithm - Stochastic Gradient Boosting (Generalized Boosted Modeling)
fit.gbm <- train(as.factor(y)~., data=train, method="gbm", metric = "Accuracy",
trControl = control)
#svm
fit.svm <- train(as.factor(y)~., data=train, method="svmRadial", metric = "Accuracy",
trControl = control)
#nnet
fit.nnet <- train(as.factor(y)~., data=train, method="nnet", metric = "Accuracy",
trControl = control)
#naive
fit.naive <- train(as.factor(y)~., data=train, method="naive_bayes", metric = "Accuracy",
trControl = control)
#extreme gradient boosting
fit.xgb <- train(as.factor(y)~., data=train, method="xgbTree", metric = "Accuracy",
trControl = control)
#bagged cart
fit.bg <- train(as.factor(y)~., data=train, method="treebag", metric = "Accuracy",
trControl = control)
#decision tree
fit.dtree <- train(as.factor(y)~., data=train, method="C5.0", metric = "Accuracy",
trControl = control)
#knn
fit.knn <- train(as.factor(y)~., data=train, method="kknn", metric = "Accuracy",
trControl = control)
stopCluster(cl)
glimpse(train)
glimpse(df$y)
glimpse(df$rating)
unique(df$rating)
unique(train$y)
#load data
df <- read.csv(file.choose(),sep = ",")
glimpse(df)
#drop columns
df <- df%>%
select(-c(X,X.1,X.2,X.3))
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#check columns
unique(df$attraction_name)
unique(df$category)
#columns to drop attraction_name, category, reviewer_name,
df <- df %>%
select(reviewer_location, review_total_contributions,
date_of_experience, review_language, review_text, rating) %>%
na.omit()
table(df$rating) # lots of 4 and 5
#clean up date
df$date_of_experience <- lubridate::mdy(df$date_of_experience)
df$year <- lubridate::year(df$date_of_experience)
df$month <- lubridate::month(df$date_of_experience)
review_ratings_df <- df %>%
select(review_text, rating)
cts_df <- df %>%
select(review_total_contributions)
cat_df <- df %>%
select(reviewer_location, review_language)
other_cat_df <- df %>%
select(year,month)
#Label Encoder
labelEncoder <-function(x){
as.numeric(factor(x))-1
}
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
cts_df <- as.data.frame(lapply(cts_df, normalize))
cat_df  <- as.data.frame(lapply(cat_df , labelEncoder))
#combine information
new_df <- cbind(cat_df,cts_df,other_cat_df,review_ratings_df)
#split into train and test
set.seed(123)
sample <- sample.split(new_df,SplitRatio = 0.75)
train <- subset(new_df,sample ==TRUE)
test <- subset(new_df, sample==FALSE)
train_df <- train %>%
select(review_text, rating)
test_df <- test %>%
select(review_text, rating)
clean_data <- function(df){
corpus_df <- VCorpus(VectorSource(df$review_text))
##Removing Punctuation
corpus_df <- tm_map(corpus_df, content_transformer(removePunctuation))
##Removing numbers
corpus_df <- tm_map(corpus_df, removeNumbers)
##Converting to lower case
#corpus_df <- tm_map(corpus_df, content_transformer(tolower))
##Removing stop words
corpus_df <- tm_map(corpus_df, content_transformer(removeWords), stopwords('english'))
##Stemming
corpus_df <- tm_map(corpus_df, stemDocument)
##Whitespace
corpus_df <- tm_map(corpus_df, stripWhitespace)
# Create Document Term Matrix
dtm_df <- DocumentTermMatrix(corpus_df)
corpus_df <- removeSparseTerms(dtm_df, 0.8)
dtm_df_matrix <- as.matrix(corpus_df)
dtm_df_matrix <- cbind(dtm_df_matrix,df$rating)
colnames(dtm_df_matrix)[ncol(dtm_df_matrix)] <- "y"
final_df <- as.data.frame(dtm_df_matrix)
final_df$y <- as.factor(final_df$y)
return (final_df)
}
train_df <- clean_data(train_df)
test_df <- clean_data(test_df)
train <- cbind(train[,c(1,2,3,4,5)], train_df)
test <- cbind(test[,c(1,2,3,4,5)], test_df)
#model training
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs = FALSE)
#glm
fit.glm <- train(as.factor(y)~., data=train, method="multinom", metric = "Accuracy",
trControl = control)
#model training
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs = TRUE)
#glm
fit.glm <- train(as.factor(y)~., data=train, method="multinom", metric = "Accuracy",
trControl = control)
train <- train %>%
na.omit()
test <- test %>%
na.omit()
#model training
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs = FALSE)
#glm
fit.glm <- train(as.factor(y)~., data=train, method="multinom", metric = "Accuracy",
trControl = control)
#model training
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs = FALSE)
#glm
fit.glm <- train(as.factor(y)~., data=train, method="multinom", metric = "Accuracy",
trControl = control)
#random forest
fit.rf <- train(as.factor(y)~., data=train, method="rf", metric = "Accuracy",
trControl = control)
#boosting algorithm - Stochastic Gradient Boosting (Generalized Boosted Modeling)
fit.gbm <- train(as.factor(y)~., data=train, method="gbm", metric = "Accuracy",
trControl = control)
#svm
fit.svm <- train(as.factor(y)~., data=train, method="svmRadial", metric = "Accuracy",
trControl = control)
#nnet
fit.nnet <- train(as.factor(y)~., data=train, method="nnet", metric = "Accuracy",
trControl = control)
#naive
fit.naive <- train(as.factor(y)~., data=train, method="naive_bayes", metric = "Accuracy",
trControl = control)
#extreme gradient boosting
fit.xgb <- train(as.factor(y)~., data=train, method="xgbTree", metric = "Accuracy",
trControl = control)
#bagged cart
fit.bg <- train(as.factor(y)~., data=train, method="treebag", metric = "Accuracy",
trControl = control)
#decision tree
fit.dtree <- train(as.factor(y)~., data=train, method="C5.0", metric = "Accuracy",
trControl = control)
#knn
fit.knn <- train(as.factor(y)~., data=train, method="kknn", metric = "Accuracy",
trControl = control)
stopCluster(cl)
results <- resamples(list(randomforest = fit.rf,
`gradient boost` = fit.gbm,
`support vector machine` = fit.svm,
baggedCart = fit.bg,
neuralnetwork = fit.nnet,
xgboost = fit.xgb,
logisticregression = fit.glm,
`decision tree` = fit.dtree,
`naive bayes` = fit.naive))
summary(results)
# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)
# test data accuracy
# Make predictions
predicted.classes <- fit.xgb %>% predict(test)
output <- confusionMatrix(data = predicted.classes, reference = test$y, mode = "everything")
# =======================================================
# packages
# =======================================================
rm(list=ls())
packages <- c('ggplot2', 'corrplot','tidyverse','dplyr','tidyr',
'caret','mlbench','caTools','scales','readxl',
'doParallel','scales','catboost', 'Matrix','lubridate')
# load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read_excel(file.choose())
View(df)
packages <- c('ggplot2', 'corrplot','tidyverse','dplyr','tidyr',
'caret','mlbench','caTools','scales','readxl',
'doParallel','scales','catboost', 'Matrix','lubridate',
'xts','TTR','forecast')
# load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df.xts <- xts(x = df$Percentage, order.by = df$Months)
str(df.xts)
df.end <- floor(0.6*length(df.xts)) #select the first 60% of the data
df.train <- df.xts[1:df.end,] #assign the first 60% of the data to the train set
df.test <- df.txt[(df.end+1):length(df.xts),] #assign the most recent 40% to the test set
df.test <- df.xts[(df.end+1):length(df.xts),] #assign the most recent 40% to the test set
df.start <- c(year(start(df.test)), month(start(df.test)))
df.end <- c(year(end(df.test)), month(end(df.test)))
df.test <- ts(as.numeric(df.test), start = df.start,
end = df.end, frequency = 12)
forecast.horizon <- length(df.test)
df.train.components <- decompose(df.train)
plot(df.train.components)
View(df.train)
shiny::runApp('Documents/Coding/R/shiny/residential_rebate')
runApp('Documents/Coding/R/shiny/residential_rebate')
rm(list = ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','shiny','shinydashboard',
'dplyr','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read_excel(file.choose())
df[is.na(df)] <- 0
area <- c(sort(unique(df$Area)))
fiscal_year <- c(sort(unique(df$`Fiscal Year`)))
rebate_type <- c(sort(unique(df$Type)))
colnms=c("July","August","September","October","November",
"December","January","February","March","April","May","June")
df$month_total <- rowSums(df[,colnms])
View(df)
df <- df %>%
select(Area, `Fiscal Year`, Type, `Type Detail`, Total)
df <- df %>%
select(Area, `Fiscal Year`, Type, `Type Detail`, month_total)
View(df)
df_test <- df %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-2020') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total))
df_test <- df %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-2020') %>%
group_by(Type) %>%
summarise(total = sum(month_total))
df_test <- df %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-2020') %>%
group_by(Type)
View(df_test)
df_test <- df %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-20') %>%
group_by(Type) %>%
summarise(total = sum(month_total))
View(df_test)
df_test <- df %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-20') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total))
View(df_test)
df_test <- df %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-20') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total)) %>%
select(Area, `Fiscal Year`,Type, `Type Detail`,total)
df_test <- df %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-20') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total)) %>%
select(Area, `Fiscal Year`,Type, `Type Detail`,total)
df_test <- df %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total)) %>%
select(Area, `Fiscal Year`,Type, `Type Detail`,total)
df_test <- df %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total)) %>%
select(`Fiscal Year`,Type, `Type Detail`,total)
df_test <- df %>%
filter(Area = "Ontario") %>%
filter(`Fiscal Year` = 'FY 2019-20') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total)) %>%
select(`Fiscal Year`,Type, `Type Detail`,total)
df_test <- df %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20') %>%
group_by(`Type Detail`)
View(df_test)
df_test <- df %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20') %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total))
df_test <- df %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20') %>%
summarise(total = sum(month_total)) %>%
group_by(`Type Detail`)
df_test <- df %>%
group_by(`Type Detail`) %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20') %>%
summarise(total = sum(month_total))
View(df_test)
df_test <- df %>%
group_by(`Type Detail`) %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20') %>%
summarise(total = sum(month_total)) %>%
select(`Fiscal Year`,Type, `Type Detail`,total)
df_test <- df %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total)) %>%
select(`Fiscal Year`,Type, `Type Detail`,total) %>%
filter(Area == "Ontario") %>%
filter(`Fiscal Year` == 'FY 2019-20')
df_test <- df %>%
group_by(`Type Detail`) %>%
summarise(total = sum(month_total))
df_test <- df %>%
group_by(Area,Type,`Fiscal Year`,`Type Detail`) %>%
filter(Area == "Ontario",`Fiscal Year` == 'FY 2019-20') %>%
summarise(total = sum(month_total))
View(df_test)
runApp('Documents/Coding/R/shiny/residential_rebate')
setwd("~/Documents/Coding/hackathon_coding_challenges/tigergraph_web_hack/visa-network-impact-cloud_data")
APP_APP <- read.csv("APP_APP.csv")
APP_SERVICE <- read.csv("APP_SERVICE.csv")
LUN <- read("LUN.csv")
SERVER_APP <- read.csv("SERVER_APP.csv")
SERVICE_MANAGER <- read.csv("SERVICE_MANAGER.csv")
SWITCH <- read.csv("SWITCH.csv")
warnings <- read.csv("warnings.csv")
LUN <- read.csv("LUN.csv")
View(LUN)
